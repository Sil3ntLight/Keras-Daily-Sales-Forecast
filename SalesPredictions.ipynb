{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load = True\n",
    "evaluate = True\n",
    "doTrain = True\n",
    "model_file = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "EVALUATION_INTERVAL = 250 \n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 10000\n",
    "future_target = 100\n",
    "STEP = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def date_parser(date_str):\n",
    "    return dt.strptime(date_str, \"%Y-%m-%d\").strftime(\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(path.exists('./logs/')):\n",
    "    shutil.rmtree('./logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_connection_str = 'mysql+pymysql://root:alpine5676@10.0.0.20:32768/kuhl_prod'\n",
    "db_connection = create_engine(db_connection_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = 'select * from `commerce_order` INNER JOIN `commerce_orderitem` ON `commerce_order`.`order_number` = `commerce_orderitem`.`order_id`;'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logdir = \"logs\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1,profile_batch = 1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_time_steps(length):\n",
    "  return list(range(-length, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  start_index = start_index + history_size\n",
    "  if end_index is None:\n",
    "    end_index = len(dataset) - target_size\n",
    "\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-history_size, i, step)\n",
    "    data.append(dataset[indices])\n",
    "\n",
    "    \n",
    "    labels.append(target[i:i+target_size])\n",
    "\n",
    "\n",
    "  return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chunks_to_df(gen):\n",
    "    chunks = []\n",
    "    for df in gen:\n",
    "        chunks.append(df)\n",
    "    return pd.concat(chunks).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  num_in = create_time_steps(len(history))\n",
    "  num_out = len(true_future)\n",
    "\n",
    "  plt.plot(num_in, np.array(history[:, 5]), label='History')\n",
    "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "           label='True Future')\n",
    "  if prediction.any():\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
    "             label='Predicted Future')\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=200):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('count')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change column names to columns that affect data. (some of features_cols are generated later on based on column 'date_placed')\n",
    "features_cols = ['upc_code', 'style_no', 'color_code', 'waist', 'size', 'dayofweek', 'month', 'dayofyear', 'weekofyear', 'quarter', 'count']\n",
    "num_cols = ['upc_code', 'style_no', 'dayofweek', 'month', 'dayofyear', 'weekofyear', 'quarter', 'count']\n",
    "#Define model\n",
    "X_num_columns= len(features_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(query, con=db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2905487 train examples\n",
      "726372 test examples\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.0._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.1._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.2._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.3._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.4._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.5._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.6._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.7._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.8._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.9._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.10._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.11._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.12._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.13._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.14._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.15._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.16._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.17._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.18._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.19._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.20._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.21._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.22._initializer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-0._resources.23._initializer\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model Created\n",
      "Train for 90797 steps\n",
      "Epoch 1/10\n",
      "90797/90797 [==============================] - 730s 8ms/step - loss: 0.2487 - accuracy: 0.8667\n",
      "Epoch 2/10\n",
      "90797/90797 [==============================] - 732s 8ms/step - loss: 0.2462 - accuracy: 0.8672\n",
      "Epoch 3/10\n",
      "18409/90797 [=====>........................] - ETA: 10:54 - loss: 0.2470 - accuracy: 0.8673"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_columns = []\n",
    "tf.random.set_seed(13)\n",
    "# make sure date column is in datetime YYYY-MM-DD format\n",
    "df['date_placed'] = df['date_placed'].astype(str) + ' 00:00:00.00' \n",
    "df['date_placed'] = pd.to_datetime(df['date_placed'])\n",
    "\n",
    "df_old = df\n",
    "# group by date, and then upc, and count how many of each upc was ordered for each date.\n",
    "df = df.groupby(by=['date_placed', 'upc_code'],as_index=False).size().to_frame('count').reset_index()\n",
    "# Fill other feature columns regarding date, determined by 'date_placed'\n",
    "df['year'] = pd.to_numeric(df['date_placed'].dt.year)\n",
    "df['dayofweek'] = pd.to_numeric(df['date_placed'].dt.dayofweek)\n",
    "df['month'] = pd.to_numeric(df['date_placed'].dt.month)\n",
    "df['dayofyear'] = pd.to_numeric(df['date_placed'].dt.dayofyear)\n",
    "df['weekofyear'] = pd.to_numeric(df['date_placed'].dt.weekofyear)\n",
    "df['quarter'] = pd.to_numeric(df['date_placed'].dt.quarter)\n",
    "df['upc_code'] = pd.to_numeric(df['upc_code']) \n",
    "df['count'] = pd.to_numeric(df['count'])\n",
    "df_old['upc_code'] = pd.to_numeric(df_old['upc_code']) \n",
    "df = df.merge(df_old[['upc_code','waist','style_no','color_code','size']], how='inner', on='upc_code')\n",
    "\n",
    "df = df.drop('date_placed', axis=1)\n",
    "\n",
    "\n",
    "waist = feature_column.categorical_column_with_vocabulary_list('waist', df['waist'].unique().tolist())\n",
    "waist_embedding = feature_column.embedding_column(waist, dimension=8)\n",
    "## color_code\n",
    "color_code = feature_column.categorical_column_with_vocabulary_list('color_code', df['color_code'].unique().tolist())\n",
    "color_code_embedding = feature_column.embedding_column(color_code, dimension=8)\n",
    "## size\n",
    "size = feature_column.categorical_column_with_vocabulary_list('size', df['size'].unique().tolist())\n",
    "size_embedding = feature_column.embedding_column(size, dimension=8)\n",
    "\n",
    "\n",
    "feature_columns.append(waist_embedding)\n",
    "feature_columns.append(color_code_embedding)    \n",
    "feature_columns.append(size_embedding)  \n",
    "# add to keras model\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "# create seperated datasets\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(test), 'test examples')\n",
    "\n",
    "train_ds = df_to_dataset(train, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "if(load):\n",
    "    model = load_model('model')\n",
    "\n",
    "if (doTrain):\n",
    "    model = Sequential()\n",
    "    model.add(feature_layer)\n",
    "    model.add(Dense(300,\n",
    "                    activation='relu'))\n",
    "    model.add(Dense(90,\n",
    "                        activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(30,\n",
    "                        activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(7,\n",
    "                        activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1,\n",
    "                        activation='linear'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    print(\"Model Created\")\n",
    "    model.fit(train_ds, epochs=EPOCHS,callbacks=[tensorboard_callback])\n",
    "    model.save('model')\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model('model')\n",
    "\n",
    "if (evaluate):\n",
    "\n",
    "    print('\\n# Evaluate on test data')\n",
    "    results = model.evaluate(test_ds)\n",
    "    print('test loss, test acc:', results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_upc(predict_df, upc):\n",
    "    predict_df = predict_df.reset_index(drop=True)\n",
    "    \n",
    "    predict_ds = df_to_dataset(predict_df, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    Predicted_sales = model.predict(predict_ds)\n",
    "    \n",
    "\n",
    "    new_predictions_series = pd.DataFrame(Predicted_sales, columns=['count'])\n",
    "    new_predictions_series = new_predictions_series.reset_index(drop=True)\n",
    "    new_predictions_df = predict_df.reset_index(drop=True)\n",
    "    new_predictions_df['count'] = new_predictions_series['count']\n",
    "\n",
    "    new_predictions_df.to_csv(\"./predictions/\" + str(upc) + \"_predicted-sales.csv\")\n",
    "    print(str(upc) + \" Done! Exported to ./predictions/\"+str(upc)+\"_predicted-sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframe to predict for\n",
    "predict_df_upc = pd.DataFrame(columns=features_cols)\n",
    "\n",
    "predict_df_upc['upc_code'] = pd.DataFrame(df['upc_code'].unique(), columns=['upc_code'])['upc_code']\n",
    "\n",
    "\n",
    "predict_df_upc = predict_df_upc.merge(df[['upc_code','waist','style_no','color_code','size']], how='inner', on='upc_code')\n",
    "\n",
    "group = predict_df_upc.groupby('upc_code')\n",
    "\n",
    "for upc, data in group:\n",
    "    predict_df = pd.DataFrame(columns=features_cols)\n",
    "    predict_df['date_placed'] = \"\"\n",
    "    predict_df['count'] = \"\"\n",
    "    predict_df_dates = pd.date_range(start=\"01/28/2020\", periods=future_target)\n",
    "    for date in predict_df_dates.values:\n",
    "        date_upc_df = data\n",
    "        date_upc_df['date_placed'] = date\n",
    "        predict_df = predict_df.append(date_upc_df, sort=True)\n",
    "    # Fill other predict columns regarding date, determined by 'date_placed'\n",
    "    predict_df['date_placed'] = pd.to_datetime(predict_df['date_placed'])\n",
    "    predict_df['year'] = pd.to_numeric(predict_df['date_placed'].dt.year)\n",
    "    predict_df['dayofweek'] = pd.to_numeric(predict_df['date_placed'].dt.dayofweek)\n",
    "    predict_df['month'] = pd.to_numeric(predict_df['date_placed'].dt.month)\n",
    "    predict_df['dayofyear'] = pd.to_numeric(predict_df['date_placed'].dt.dayofyear)\n",
    "    predict_df['weekofyear'] = pd.to_numeric(predict_df['date_placed'].dt.weekofyear)\n",
    "    predict_df['quarter'] = pd.to_numeric(predict_df['date_placed'].dt.quarter)\n",
    "    predict_df['waist'] = predict_df['waist_y']\n",
    "    predict_df['style_no'] = predict_df['style_no_y']\n",
    "    predict_df['color_code'] = predict_df['color_code_y']\n",
    "    predict_df['size'] = predict_df['size_y']\n",
    "\n",
    "    predict_df.drop('style_no_x',inplace=True, axis=1)\n",
    "    predict_df.drop('style_no_y',inplace=True, axis=1)\n",
    "    predict_df.drop('color_code_x',inplace=True, axis=1)\n",
    "    predict_df.drop('color_code_y',inplace=True, axis=1)\n",
    "    predict_df.drop('waist_x',inplace=True, axis=1)\n",
    "    predict_df.drop('waist_y',inplace=True, axis=1)\n",
    "    predict_df.drop('size_x',inplace=True, axis=1)\n",
    "    predict_df.drop('size_y',inplace=True, axis=1)\n",
    "    predict_df.drop('date_placed',inplace=True, axis=1)\n",
    "    predict_df['waist'] = predict_df['waist'].fillna(\"_\")\n",
    "    predict_df['color_code'] = predict_df['color_code'].fillna(\"_\")\n",
    "    predict_df['size'] = predict_df['size'].fillna(\"_\")\n",
    "    predict_df['style_no'] = predict_df['style_no'].fillna(\"_\")\n",
    "    \n",
    "    predict_df = predict_df.drop_duplicates(subset=['year','dayofyear'])\n",
    "\n",
    "    predict_per_upc(predict_df, upc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
